"""
ãƒ‡ãƒ¼ã‚¿å¤‰æ›ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
æ™‚é–“å¤‰æ›ã€åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã®çµ±ä¸€ã€ãƒ‡ãƒ¼ã‚¿å‹ã®æ­£è¦åŒ–
"""

import pandas as pd
import numpy as np
from typing import List, Dict
from config import INPUT_TIMEZONE, OUTPUT_TIMEZONE, DEBUG


def convert_timestamp_to_utc(timestamp_str: str) -> pd.Timestamp:
    """ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ–‡å­—åˆ—ã‚’UTCã®naive datetimeã«å¤‰æ›
    - tzãªã—(naive)ã¯ INPUT_TIMEZONE ã¨ã—ã¦è§£é‡ˆ
    - æœ€çµ‚çš„ã« OUTPUT_TIMEZONE ã«å¤‰æ›ã—ã¦ tzæƒ…å ±ã‚’å¤–ã™
    """
    if not timestamp_str:
        return None
    try:
        dt = pd.to_datetime(timestamp_str, errors='coerce')
        if pd.isna(dt):
            return None
        # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³æ­£è¦åŒ–
        if dt.tz is None:
            # å…¥åŠ›ãŒnaive â†’ å…¥åŠ›TZã¨ã—ã¦localize
            dt = dt.tz_localize(INPUT_TIMEZONE)
        # å‡ºåŠ›TZã¸å¤‰æ›
        dt = dt.tz_convert(OUTPUT_TIMEZONE)
        # æœ€å¾Œã«naiveåŒ–
        return dt.tz_localize(None)
    except Exception as e:
        if DEBUG:
            print(f"   âš ï¸ æ™‚é–“å¤‰æ›ã‚¨ãƒ©ãƒ¼: {timestamp_str!r} -> {e}")
        return None


def normalize_numeric_value(value) -> float:
    """æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–"""
    if value is None or value == '':
        return None
    
    try:
        return float(value)
    except (ValueError, TypeError):
        return None


def convert_records_to_dataframe(records: List[Dict]) -> pd.DataFrame:
    """ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’DataFrameã«å¤‰æ›ã—ã¦æ­£è¦åŒ–"""
    if not records:
        return pd.DataFrame()
    
    df = pd.DataFrame(records)

    if DEBUG:
        print(f"   ğŸ”„ {len(df)} ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å¤‰æ›ä¸­...")
    
    # æ™‚é–“ã‚«ãƒ©ãƒ ã®å¤‰æ›
    time_columns = ['start_time', 'end_time', 'point_time']
    for col in time_columns:
        if col in df.columns:
            df[col] = df[col].apply(convert_timestamp_to_utc)
    
    # æ•°å€¤ã‚«ãƒ©ãƒ ã®å¤‰æ›ï¼ˆTimelineç”¨ï¼‰
    numeric_columns = [
        'latitude', 'longitude', 'activity_distanceMeters', 
        'visit_probability', 'activity_probability'
    ]
    
    # GPXç”¨ã®è¿½åŠ æ•°å€¤ã‚«ãƒ©ãƒ 
    gpx_numeric_columns = [
        '_gpx_elevation', '_gpx_speed', '_gpx_point_sequence'
    ]
    
    all_numeric_columns = numeric_columns + gpx_numeric_columns
    
    for col in all_numeric_columns:
        if col in df.columns:
            df[col] = df[col].apply(normalize_numeric_value)
    
    # NaN ã‚’ None ã«ç½®æ›
    df = df.replace({np.nan: None})
    
    if DEBUG:
        print(f"   âœ… ãƒ‡ãƒ¼ã‚¿å¤‰æ›å®Œäº†")
    
    return df


def _to_naive_utc(series: pd.Series) -> pd.Series:
    """æ··åœ¨ãƒ‡ãƒ¼ã‚¿(æ–‡å­—åˆ—/Timestamp/naive/tz-aware)ã‚’çµ±ä¸€
    - naiveâ†’INPUT_TIMEZONE ã§localize
    - OUTPUT_TIMEZONEã¸å¤‰æ›â†’naiveåŒ–
    """
    # å€‹ã€…ã«å‡¦ç†ã—ã¦ã‚·ãƒªãƒ¼ã‚ºã‚’è¿”ã™
    def _conv(x):
        if x is None or (isinstance(x, float) and pd.isna(x)):
            return None
        try:
            if not isinstance(x, pd.Timestamp):
                x = pd.to_datetime(x, errors='coerce')
            if pd.isna(x):
                return None
            if x.tz is None:
                x = x.tz_localize(INPUT_TIMEZONE)
            x = x.tz_convert(OUTPUT_TIMEZONE)
            return x.tz_localize(None)
        except Exception:
            return None
    return series.apply(_conv)


def sort_dataframe_by_time(df: pd.DataFrame) -> pd.DataFrame:
    """DataFrameã‚’æ™‚é–“é †ã«ã‚½ãƒ¼ãƒˆ (å­˜åœ¨ã—ãªã„åˆ—ã‚’å®‰å…¨ã«æ‰±ã†)"""
    if df.empty:
        return df

    if DEBUG:
        print("   ğŸ”„ æ™‚é–“é †ã‚½ãƒ¼ãƒˆä¸­...")

    # ã™ã¹ã¦ã®æ™‚é–“åˆ—ã‚’UTC naiveãªdatetime64[ns]ã«çµ±ä¸€ï¼ˆå­˜åœ¨ã™ã‚‹ã‚‚ã®ã®ã¿ï¼‰
    for col in ["point_time", "start_time", "end_time"]:
        if col in df.columns:
            df[col] = _to_naive_utc(df[col])

    # å„åˆ—ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ NaT ã‚·ãƒªãƒ¼ã‚ºã‚’ç”¨æ„
    def _safe_series(name: str) -> pd.Series:
        if name in df.columns:
            return df[name]
        # é•·ã•0å¯¾ç­–ï¼ˆç©ºDFã¯æœ€åˆã«returnã—ã¦ã„ã‚‹ã®ã§ len(df)>0 å‰æï¼‰
        return pd.Series([pd.NaT] * len(df), name=name)

    point_series = _safe_series("point_time")
    start_series = _safe_series("start_time")
    end_series = _safe_series("end_time")

    # å„ªå…ˆé †ä½: point_time > start_time > end_time
    sort_time = point_series.fillna(start_series).fillna(end_series)

    # ã‚½ãƒ¼ãƒˆå®Ÿè¡Œ
    if not sort_time.isna().all():
        sorted_indices = sort_time.sort_values().index
        df_sorted = df.loc[sorted_indices].reset_index(drop=True)

        if DEBUG:
            print(f"   âœ… æ™‚é–“é †ã‚½ãƒ¼ãƒˆå®Œäº†")

        return df_sorted
    else:
        if DEBUG:
            print("   âš ï¸ ã‚½ãƒ¼ãƒˆå¯èƒ½ãªæ™‚é–“ãƒ‡ãƒ¼ã‚¿ãªã—")
        return df


def combine_dataframes(dataframes: List[pd.DataFrame]) -> pd.DataFrame:
    """è¤‡æ•°ã®DataFrameã‚’çµåˆ
    - ç©ºã®DataFrameã¯é™¤å¤–
    - å…¨è¡ŒNAã®åˆ—ã¯ä¸€æ™‚çš„ã«é™¤å»ã—ã¦ã‹ã‚‰çµåˆï¼ˆpandasã®å°†æ¥ä»•æ§˜å¤‰æ›´ã«å‚™ãˆã‚‹ï¼‰
    """
    if not dataframes:
        return pd.DataFrame()

    # ç©ºDFã‚’é™¤å¤–ã—ã€å…¨NAåˆ—ã‚’ãƒ‰ãƒ­ãƒƒãƒ—ï¼ˆçµåˆå¾Œã«å¿…è¦ãªåˆ—ã¯ä»–DFã«å€¤ãŒã‚ã‚Œã°è‡ªå‹•ã§æƒã†ï¼‰
    valid_dfs: List[pd.DataFrame] = []
    for df in dataframes:
        if isinstance(df, pd.DataFrame) and not df.empty:
            cleaned = df.dropna(axis=1, how='all')
            valid_dfs.append(cleaned)

    if not valid_dfs:
        return pd.DataFrame()

    if DEBUG:
        total_records = sum(len(df) for df in valid_dfs)
        print(f"ğŸ”— {len(valid_dfs)}å€‹ã®DataFrameã‚’çµåˆä¸­... (ç·{total_records}ãƒ¬ã‚³ãƒ¼ãƒ‰)")
    
    combined_df = pd.concat(valid_dfs, ignore_index=True, copy=False)

    if DEBUG:
        print(f"âœ… çµåˆå®Œäº†: {len(combined_df)} ãƒ¬ã‚³ãƒ¼ãƒ‰")
    
    return combined_df


def get_dataframe_summary(df: pd.DataFrame) -> Dict:
    """DataFrameã®è¦ç´„æƒ…å ±ã‚’å–å¾— (æ™‚é–“åˆ—ãŒæ¬ ã‘ã‚‹ã‚±ãƒ¼ã‚¹ã«å¯¾å¿œ)"""
    if df.empty:
        return {"total_records": 0}

    # å­˜åœ¨ã™ã‚‹æ™‚é–“åˆ—ã®ã¿é€£çµ
    time_series_list = []
    for name in ["point_time", "start_time", "end_time"]:
        s = df.get(name)
        if s is not None:
            time_series_list.append(s)
    if time_series_list:
        _times = pd.concat(time_series_list)
    else:
        _times = pd.Series([], dtype="datetime64[ns]")

    summary = {
        "total_records": len(df),
        "data_types": df['type'].value_counts().to_dict() if 'type' in df.columns else {},
        "users": df['username'].value_counts().to_dict() if 'username' in df.columns else {},
        "time_range": {
            "start": _times.min() if not _times.empty else None,
            "end": _times.max() if not _times.empty else None,
        },
        "location_records": df[['latitude', 'longitude']].notna().all(axis=1).sum() if all(col in df.columns for col in ['latitude', 'longitude']) else 0
    }

    # GPXãƒ‡ãƒ¼ã‚¿ã®ç‰¹åˆ¥çµ±è¨ˆ
    gpx_data = df[df['type'].str.startswith('gpx', na=False)] if 'type' in df.columns else pd.DataFrame()
    if not gpx_data.empty:
        summary["gpx_stats"] = {
            "total_gpx_records": len(gpx_data),
            "gpx_data_sources": gpx_data['_gpx_data_source'].value_counts().to_dict() if '_gpx_data_source' in gpx_data.columns else {},
            "gpx_tracks": gpx_data['_gpx_track_name'].nunique() if '_gpx_track_name' in gpx_data.columns else 0,
            "elevation_range": {
                "min": gpx_data['_gpx_elevation'].min() if '_gpx_elevation' in gpx_data.columns else None,
                "max": gpx_data['_gpx_elevation'].max() if '_gpx_elevation' in gpx_data.columns else None
            } if '_gpx_elevation' in gpx_data.columns else None
        }

    return summary
