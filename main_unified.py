#!/usr/bin/env python3
"""
Google Timeline & GPX Parser - ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨GPXãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦çµ±åˆCSVã‚’å‡ºåŠ›
"""

import sys
import os
from datetime import datetime

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from modules.file_handler import find_all_files, load_json_file, get_username_from_filename, validate_json_data, validate_gpx_file
from modules.json_parser import parse_json_data
from modules.gpx_parser import parse_gpx_file
from modules.data_converter import convert_records_to_dataframe, sort_dataframe_by_time, combine_dataframes
from modules.csv_exporter import export_to_csv, print_summary, validate_output_data
from config import OUTPUT_FILE, DEBUG
from modules import kml_parser
import pandas as pd


def process_json_files(json_files):
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†"""
    processed_dataframes = []
    processed_count = 0
    
    for i, json_file in enumerate(json_files, 1):
        print(f"\n[JSON {i}/{len(json_files)}] {os.path.basename(json_file)}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        data = load_json_file(json_file)
        if data is None:
            continue
        
        # ãƒ‡ãƒ¼ã‚¿å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        if not validate_json_data(data):
            continue
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼åç”Ÿæˆ
        username = get_username_from_filename(json_file)
        
        # JSONè§£æ
        records = parse_json_data(data, username)
        if not records:
            continue
        
        # DataFrameå¤‰æ›
        df = convert_records_to_dataframe(records)
        if df.empty:
            continue
        
        processed_dataframes.append(df)
        processed_count += 1
    
    return processed_dataframes, processed_count


def process_gpx_files(gpx_files):
    """GPXãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†"""
    processed_dataframes = []
    processed_count = 0
    
    for i, gpx_file in enumerate(gpx_files, 1):
        print(f"\n[GPX {i}/{len(gpx_files)}] {os.path.basename(gpx_file)}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        if not validate_gpx_file(gpx_file):
            continue
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼åç”Ÿæˆ
        username = get_username_from_filename(gpx_file)
        
        # GPXè§£æ
        records = parse_gpx_file(gpx_file, username)
        if not records:
            continue
        
        # DataFrameå¤‰æ›
        df = convert_records_to_dataframe(records)
        if df.empty:
            continue
        
        processed_dataframes.append(df)
        processed_count += 1
    
    return processed_dataframes, processed_count


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸŒ Google Timeline & GPX Parser")
    print("=" * 50)
    
    start_time = datetime.now()
    
    # 1. å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œç´¢
    print("\nğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ä¸­...")
    all_files = find_all_files()
    json_files = all_files['json']
    gpx_files = all_files['gpx']
    
    total_files = len(json_files) + len(gpx_files)
    
    if total_files == 0:
        print("âŒ å‡¦ç†ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print(f"   dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª({os.path.abspath('data')})ã«JSONã¾ãŸã¯GPXãƒ•ã‚¡ã‚¤ãƒ«ã‚’é…ç½®ã—ã¦ãã ã•ã„")
        return
    
    print(f"ğŸ“Š ç™ºè¦‹ãƒ•ã‚¡ã‚¤ãƒ«: JSON {len(json_files)}å€‹, GPX {len(gpx_files)}å€‹")
    
    # 2. å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã‚’å‡¦ç†
    all_dataframes = []
    total_processed = 0
    
    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
    if json_files:
        print(f"\nğŸ”„ JSONå‡¦ç†: {len(json_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­...")
        json_dataframes, json_processed = process_json_files(json_files)
        all_dataframes.extend(json_dataframes)
        total_processed += json_processed
        print(f"âœ… JSONå‡¦ç†å®Œäº†: {json_processed}/{len(json_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†")
    
    # GPXãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
    if gpx_files:
        print(f"\nğŸ”ï¸ GPXå‡¦ç†: {len(gpx_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­...")
        gpx_dataframes, gpx_processed = process_gpx_files(gpx_files)
        all_dataframes.extend(gpx_dataframes)
        total_processed += gpx_processed
        print(f"âœ… GPXå‡¦ç†å®Œäº†: {gpx_processed}/{len(gpx_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†")
    
    # KMLãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
    kml_files = all_files.get('kml', [])
    if kml_files:
        print(f"\nğŸ—ºï¸ KML/KMZå‡¦ç†: {len(kml_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­...")
        kml_processed = 0
        for i, kml_file in enumerate(kml_files, 1):
            print(f"[KML {i}/{len(kml_files)}] {os.path.basename(kml_file)}")
            ext = os.path.splitext(kml_file)[1].lower()
            if ext in {".kml", ".kmz"}:
                try:
                    username = get_username_from_filename(kml_file)
                    records = kml_parser.parse_kml_file(kml_file, username=username)
                    df = convert_records_to_dataframe(records)  # â† ã“ã“ã§æ­£è¦åŒ–
                    if not df.empty:
                        all_dataframes.append(df)
                        kml_processed += 1
                except Exception as exc:
                    print(f"   âŒ KMLè§£æã‚¨ãƒ©ãƒ¼: {exc}")
        print(f"âœ… KML/KMZå‡¦ç†å®Œäº†: {kml_processed}/{len(kml_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†")
    
    # 3. ãƒ‡ãƒ¼ã‚¿çµ±åˆ
    if not all_dataframes:
        print("\nâŒ æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    print(f"\nğŸ”— {total_processed}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆä¸­...")
    combined_df = combine_dataframes(all_dataframes)
    
    if combined_df.empty:
        print("âŒ çµ±åˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
        return
    
    # 4. æ™‚é–“ã‚½ãƒ¼ãƒˆ
    print("\nâ° æ™‚é–“é †ã‚½ãƒ¼ãƒˆä¸­...")
    sorted_df = sort_dataframe_by_time(combined_df)
    
    # 5. å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
    if not validate_output_data(sorted_df):
        print("âŒ å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    # 6. CSVå‡ºåŠ›
    print(f"\nğŸ’¾ CSVå‡ºåŠ›ä¸­...")
    output_file = export_to_csv(sorted_df, OUTPUT_FILE)
    
    if output_file:
        # 7. çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
        end_time = datetime.now()
        processing_time = end_time - start_time
        
        print_summary(sorted_df, output_file)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ
        print("\nğŸ“ˆ ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ:")
        type_counts = sorted_df['type'].value_counts()
        for data_type, count in type_counts.items():
            print(f"   - {data_type}: {count:,}ä»¶")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ¥çµ±è¨ˆï¼ˆGPXãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆï¼‰
        if '_gpx_data_source' in sorted_df.columns:
            gpx_data = sorted_df[sorted_df['type'].str.startswith('gpx', na=False)]
            if not gpx_data.empty:
                print("\nğŸ”ï¸ GPXãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ¥çµ±è¨ˆ:")
                source_counts = gpx_data['_gpx_data_source'].value_counts()
                for source, count in source_counts.items():
                    print(f"   - {source}: {count:,}ä»¶")
        
        print(f"\nâ±ï¸ å‡¦ç†æ™‚é–“: {processing_time.total_seconds():.2f}ç§’")
        print("ğŸ‰ çµ±åˆå‡¦ç†å®Œäº†!")
    else:
        print("âŒ CSVå‡ºåŠ›ã«å¤±æ•—ã—ã¾ã—ãŸ")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        sys.exit(1)
