#!/usr/bin/env python3
"""
Google Timeline Parser - ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦çµ±åˆCSVã‚’å‡ºåŠ›
"""

import sys
import os
from datetime import datetime

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from modules.file_handler import find_json_files, load_json_file, get_username_from_filename, validate_json_data
from modules.json_parser import parse_json_data
from modules.data_converter import convert_records_to_dataframe, sort_dataframe_by_time, combine_dataframes
from modules.csv_exporter import export_to_csv, print_summary, validate_output_data
from config import OUTPUT_FILE, DEBUG


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸŒ Google Timeline Parser")
    print("=" * 50)
    
    start_time = datetime.now()
    
    # 1. JSONãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œç´¢
    print("\nğŸ“ JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ä¸­...")
    json_files = find_json_files()
    
    if not json_files:
        print("âŒ å‡¦ç†ã™ã‚‹JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print(f"   dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª({os.path.abspath('data')})ã«JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’é…ç½®ã—ã¦ãã ã•ã„")
        return
    
    # 2. å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    print(f"\nğŸ”„ {len(json_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­...")
    all_dataframes = []
    processed_files = 0
    
    for i, json_file in enumerate(json_files, 1):
        print(f"\n[{i}/{len(json_files)}] {os.path.basename(json_file)}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        data = load_json_file(json_file)
        if data is None:
            continue
        
        # ãƒ‡ãƒ¼ã‚¿å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        if not validate_json_data(data):
            if DEBUG:
                print("   âŒ ç„¡åŠ¹ãªãƒ‡ãƒ¼ã‚¿å½¢å¼")
            continue
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼åç”Ÿæˆ
        username = get_username_from_filename(json_file)
        
        # JSONè§£æ
        records = parse_json_data(data, username)
        if not records:
            continue
        
        # DataFrameå¤‰æ›
        df = convert_records_to_dataframe(records)
        if df.empty:
            continue
        
        all_dataframes.append(df)
        processed_files += 1
        
        if DEBUG:
            print(f"   âœ… {len(df)} ãƒ¬ã‚³ãƒ¼ãƒ‰æŠ½å‡ºå®Œäº†")
    
    # 3. ãƒ‡ãƒ¼ã‚¿çµ±åˆ
    if not all_dataframes:
        print("\nâŒ æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    print(f"\nğŸ”— {processed_files}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆä¸­...")
    combined_df = combine_dataframes(all_dataframes)
    
    if combined_df.empty:
        print("âŒ çµ±åˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
        return
    
    # 4. æ™‚é–“ã‚½ãƒ¼ãƒˆ
    print("\nâ° æ™‚é–“é †ã‚½ãƒ¼ãƒˆä¸­...")
    sorted_df = sort_dataframe_by_time(combined_df)
    
    # 5. å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
    if not validate_output_data(sorted_df):
        print("âŒ å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    # 6. CSVå‡ºåŠ›
    print(f"\nğŸ’¾ CSVå‡ºåŠ›ä¸­...")
    output_file = export_to_csv(sorted_df, OUTPUT_FILE)
    
    if output_file:
        # 7. çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
        end_time = datetime.now()
        processing_time = end_time - start_time
        
        print_summary(sorted_df, output_file)
        print(f"\nâ±ï¸ å‡¦ç†æ™‚é–“: {processing_time.total_seconds():.2f}ç§’")
        print("ğŸ‰ å‡¦ç†å®Œäº†!")
    else:
        print("âŒ CSVå‡ºåŠ›ã«å¤±æ•—ã—ã¾ã—ãŸ")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        if DEBUG:
            import traceback
            traceback.print_exc()
        sys.exit(1)
